{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "IXyyUE2dU7ZJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK data\n",
        "\n",
        "# Downloads a pre-trained model that helps split text into individual words and sentences\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "# Downloads a list of common English words (like 'the', 'a', 'is') that usually don't carry significant meaning.\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eMZJL4vWDAM",
        "outputId": "aa199b99-af87-45b3-e431-642121743948"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_cleaner(text):\n",
        "    \"\"\"\n",
        "    Clean and tokenize text by:\n",
        "    1. Removing punctuation\n",
        "    2. Converting to lowercase\n",
        "    3. Tokenizing into words\n",
        "    \"\"\"\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize into words\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "7o-MzFBXX6r9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_frequency_counter(text, remove_stopwords):\n",
        "    \"\"\"\n",
        "    Count word frequencies in text, optionally removing stop words\n",
        "    \"\"\"\n",
        "    # Clean and tokenize the text\n",
        "    tokens = text_cleaner(text)\n",
        "\n",
        "    # Remove stopwords if requested\n",
        "    if remove_stopwords:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        #print(stop_words)\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Count word frequencies\n",
        "    word_counts = Counter(tokens)\n",
        "\n",
        "    return word_counts\n"
      ],
      "metadata": {
        "id": "Wh0QxXsmX8xR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Sample text - you can replace this with reading from a file\n",
        "    sample_text = \"\"\"\n",
        "                  Deep into that darkness peering, long I stood there wondering, fearing,\n",
        "                  Doubting, dreaming dreams no mortal ever dared to dream before;\n",
        "                  But the silence was unbroken, and the stillness gave no token,\n",
        "                  And the only word there spoken was the whispered word, ‘Lenore!’\n",
        "                  \"\"\"\n",
        "\n",
        "    print(\"=== Text Cleaning and Tokenization ===\")\n",
        "    cleaned_tokens = text_cleaner(sample_text)\n",
        "    print(\"Cleaned and tokenized text:\")\n",
        "    print(cleaned_tokens)  # Print first the tokens\n",
        "    print(\"------------------------------------------\\n\")\n",
        "\n",
        "    print(\"=== Word Frequency Count ===\")\n",
        "    print(\"With stopwords:\")\n",
        "    freq_with_stopwords = word_frequency_counter(sample_text, remove_stopwords=False)\n",
        "    print(freq_with_stopwords.most_common(5))\n",
        "\n",
        "    print(\"\\nWithout stopwords:\")\n",
        "    freq_without_stopwords = word_frequency_counter(sample_text, remove_stopwords=True)\n",
        "    print(freq_without_stopwords.most_common(5))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYWwERqyUEi4",
        "outputId": "d5b446f4-5dce-4bbe-bd7f-ae1baf9508fe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Text Cleaning and Tokenization ===\n",
            "Cleaned and tokenized text:\n",
            "['deep', 'into', 'that', 'darkness', 'peering', 'long', 'i', 'stood', 'there', 'wondering', 'fearing', 'doubting', 'dreaming', 'dreams', 'no', 'mortal', 'ever', 'dared', 'to', 'dream', 'before', 'but', 'the', 'silence', 'was', 'unbroken', 'and', 'the', 'stillness', 'gave', 'no', 'token', 'and', 'the', 'only', 'word', 'there', 'spoken', 'was', 'the', 'whispered', 'word', '‘', 'lenore', '’']\n",
            "------------------------------------------\n",
            "\n",
            "=== Word Frequency Count ===\n",
            "With stopwords:\n",
            "[('the', 4), ('there', 2), ('no', 2), ('was', 2), ('and', 2)]\n",
            "\n",
            "Without stopwords:\n",
            "[('word', 2), ('deep', 1), ('darkness', 1), ('peering', 1), ('long', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yVjo-XQyYQJg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}